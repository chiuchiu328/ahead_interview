{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import fcs data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_columns', 40)\n",
    "used_cols = {   \n",
    "    'SSC-W': 1,\n",
    "    'SSC-A': 1,\n",
    "    'SSC-H': 1,\n",
    "    'FSC-W': 1,\n",
    "    'FSC-A': 1,\n",
    "    'FSC-H': 1,\n",
    "    'Time': 0,\n",
    "    'FJComp-BUV737-A': 1,\n",
    "    'FJComp-APC-A': 1,\n",
    "    'FJComp-BV711-A': 1,\n",
    "    'FJComp-BB700-P-A': 1,\n",
    "    'FJComp-BB630-A': 1,\n",
    "    'FJComp-BUV395-A': 1,\n",
    "    'FJComp-BUV563-A': 1,\n",
    "    'FJComp-BV480-A': 1,\n",
    "    'FJComp-BV421-A': 1,\n",
    "    'FJComp-BV650-A': 1,\n",
    "    'FJComp-BYG584-A': 1,\n",
    "    'FJComp-PE-CF594-A': 1,\n",
    "    'FJComp-BUV615-P-A': 1,\n",
    "    'FJComp-BUV805-A': 1,\n",
    "    'FJComp-BYG790-A': 1,\n",
    "    'FJComp-PE-Cy5.5-A': 1,\n",
    "    'FJComp-BV570-A': 1,\n",
    "    'FJComp-BUV496-A': 1,\n",
    "    'FJComp-BV605-A': 1,\n",
    "    'FJComp-BB660-P-A': 0,\n",
    "    'FJComp-BV786-A': 1,\n",
    "    'FJComp-APC-R700-A': 1,\n",
    "    'FJComp-BUV661-A': 0,\n",
    "    'FJComp-BYG670-A': 1,\n",
    "    'FJComp-BV750-P-A': 1,\n",
    "    'FJComp-FITC-A': 1,\n",
    "    'FJComp-APC-H7-A': 1,\n",
    "    'FJComp-BB790-P-A': 0,\n",
    "    'user_id': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# Directory containing FCS files\n",
    "directory_path = './data/raw_fcs'\n",
    "user_id_pattern = 'EU_\\d+'\n",
    "# Iterate through files in the directory\n",
    "info = []\n",
    "df_all = []\n",
    "print(len(os.listdir(directory_path)))\n",
    "for second_dir in os.listdir(directory_path):\n",
    "    second_path = os.path.join(directory_path, second_dir)\n",
    "\n",
    "    if os.path.isdir(second_path) and 'DS_Store' not in second_dir:\n",
    "        file_name = os.listdir(second_path)[0]\n",
    "    else:\n",
    "        continue\n",
    "    if file_name.endswith('.fcs'):\n",
    "        file_path = os.path.join(second_path, file_name)\n",
    "        try:\n",
    "            fcs_file = flowio.FlowData(file_path)\n",
    "            events = fcs_file.events\n",
    "            \n",
    "            npy_data = np.reshape(events, (-1, fcs_file.channel_count))\n",
    "            \n",
    "            column = [fcs_file.channels[str(i+1)]['PnN'] for i in range(fcs_file.channel_count)]\n",
    "            user_id = re.search(user_id_pattern, second_dir).group()\n",
    "            df = pd.DataFrame(npy_data, columns=column)\n",
    "            if user_id:\n",
    "                df['user_id'] = user_id\n",
    "            else:\n",
    "                df['user_id'] = -1\n",
    "            info.append(df.describe())\n",
    "            df_all.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {str(e)}\")\n",
    "\n",
    "df_all = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look into the data\n",
    "## basic concept\n",
    "\\# people:40\n",
    "cell infomation in person:  \n",
    "min \\#: 680  \n",
    "max \\#: 363314  \n",
    "mean: 126318  \n",
    "25%:17350  \n",
    "50%:90805  \n",
    "75%:203955  \n",
    "1. the range of the cell number is wide.\n",
    "2. question: is the number matter? or is the percentage of the certain cell more important?\n",
    "3. ans: both, and maybe the ratio looks like this A/A+B. or A+B/All\n",
    "4. feature selection sould be more careful\n",
    "## the data value\n",
    "in each columns, the value represent the how strong the light it is  \n",
    "1. some of the column sould using log\n",
    "2. all value shoud apply minmax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FSC-A', 'FSC-H', 'FSC-W', 'SSC-A', 'SSC-H', 'SSC-W', 'FJComp-APC-A',\n",
      "       'FJComp-APC-H7-A', 'FJComp-APC-R700-A', 'FJComp-BB630-A',\n",
      "       'FJComp-BB660-P-A', 'FJComp-BB700-P-A', 'FJComp-BB790-P-A',\n",
      "       'FJComp-BUV395-A', 'FJComp-BUV496-A', 'FJComp-BUV563-A',\n",
      "       'FJComp-BUV615-P-A', 'FJComp-BUV661-A', 'FJComp-BUV737-A',\n",
      "       'FJComp-BUV805-A', 'FJComp-BV421-A', 'FJComp-BV480-A', 'FJComp-BV570-A',\n",
      "       'FJComp-BV605-A', 'FJComp-BV650-A', 'FJComp-BV711-A',\n",
      "       'FJComp-BV750-P-A', 'FJComp-BV786-A', 'FJComp-BYG584-A',\n",
      "       'FJComp-BYG670-A', 'FJComp-BYG790-A', 'FJComp-FITC-A',\n",
      "       'FJComp-PE-CF594-A', 'FJComp-PE-Cy5.5-A', 'Time', 'user_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_all[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_lasser in df_all[0].columns:\n",
    "    if target_lasser == 'user_id':\n",
    "        continue\n",
    "    print(target_lasser)\n",
    "    target = [i[target_lasser] for i in info]\n",
    "    df_targer = pd.concat(target, axis=1)\n",
    "    df_targer.columns = list(range(len(df_targer.columns)))\n",
    "    df_targer = df_targer.T\n",
    "\n",
    "    def display_info(col):\n",
    "        display(df_targer[col].describe())\n",
    "    display_cols = ['max', 'min', 'mean']\n",
    "\n",
    "    display_info(display_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "should_be_log = []\n",
    "for i in df_all:\n",
    "    if 'user' in i or 'Time' in i:\n",
    "        continue\n",
    "    des = df_all[i].describe()\n",
    "    if des['min'] > 0: \n",
    "        value = math.log10(des['max']) - math.log10(des['min'])\n",
    "    else:\n",
    "        value = math.log10(des['max']) + math.log10(-des['min'])\n",
    "    if value > 3:\n",
    "        should_be_log.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[should_be_log] = df_all[should_be_log].applymap(lambda x: math.log10(x) if x >= 1 else -math.log10(-x) if x <=-1 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "drop_cols = [i for i in df_all if used_cols[i] == 0]\n",
    "minmax_data = df_all.drop(columns=drop_cols)\n",
    "input_dim = len(minmax_data.columns)\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04651723682738976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "minmax_data = minmax.fit_transform(minmax_data)\n",
    "minmax_permutation = np.random.permutation(minmax_data)\n",
    "train_minmax = minmax_permutation[:int(len(minmax_data)*0.8)]\n",
    "test_minmax = minmax_permutation[int(len(minmax_data)*0.8):]\n",
    "minmax_data_var = np.var(train_minmax)\n",
    "# minmax.data_max_\n",
    "print(minmax_data_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grouping the data as cell infomation\n",
    "using the vqvae code book as the grouping index. try different grouping number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 15:37:03.667406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(input_dim, latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(encoder_inputs)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    encoder_outputs = layers.Dense(latent_dim)(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(output_dim, latent_dim=16):\n",
    "    latent_inputs = keras.Input(shape=get_encoder(output_dim, latent_dim).output.shape[1:])\n",
    "    x = layers.Dense(64, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    decoder_outputs = layers.Dense(output_dim)(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vq_vae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 31)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                13392     \n",
      "                                                                 \n",
      " vector_quantizer (VectorQu  (None, 16)                1024      \n",
      " antizer)                                                        \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 31)                13407     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27823 (108.68 KB)\n",
      "Trainable params: 27823 (108.68 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_vqvae(input_dim, latent_dim=16, num_embeddings=64):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(input_dim, latent_dim)\n",
    "    decoder = get_decoder(input_dim, latent_dim)\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae(31).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, input_dim, latent_dim=32, num_embeddings=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(input_dim, self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1234/1234 [==============================] - 16s 12ms/step - loss: 6.2524 - reconstruction_loss: 0.5046 - vqvae_loss: 5.1669\n",
      "Epoch 2/20\n",
      "1234/1234 [==============================] - 14s 12ms/step - loss: 0.6526 - reconstruction_loss: 0.2432 - vqvae_loss: 0.3898\n",
      "Epoch 3/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.6824 - reconstruction_loss: 0.2112 - vqvae_loss: 0.4698\n",
      "Epoch 4/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.8168 - reconstruction_loss: 0.2103 - vqvae_loss: 0.6060\n",
      "Epoch 5/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.8065 - reconstruction_loss: 0.2068 - vqvae_loss: 0.5988\n",
      "Epoch 6/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.7671 - reconstruction_loss: 0.2071 - vqvae_loss: 0.5606\n",
      "Epoch 7/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.7882 - reconstruction_loss: 0.2075 - vqvae_loss: 0.5801\n",
      "Epoch 8/20\n",
      "1234/1234 [==============================] - 16s 13ms/step - loss: 0.8022 - reconstruction_loss: 0.2051 - vqvae_loss: 0.5968\n",
      "Epoch 9/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.8130 - reconstruction_loss: 0.2051 - vqvae_loss: 0.6084\n",
      "Epoch 10/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.7758 - reconstruction_loss: 0.2066 - vqvae_loss: 0.5689\n",
      "Epoch 11/20\n",
      "1234/1234 [==============================] - 14s 12ms/step - loss: 0.7391 - reconstruction_loss: 0.2045 - vqvae_loss: 0.5344\n",
      "Epoch 12/20\n",
      "1234/1234 [==============================] - 16s 13ms/step - loss: 0.7065 - reconstruction_loss: 0.2036 - vqvae_loss: 0.5025\n",
      "Epoch 13/20\n",
      "1234/1234 [==============================] - 16s 13ms/step - loss: 0.6968 - reconstruction_loss: 0.2024 - vqvae_loss: 0.4943\n",
      "Epoch 14/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.6686 - reconstruction_loss: 0.2021 - vqvae_loss: 0.4663\n",
      "Epoch 15/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.6125 - reconstruction_loss: 0.2004 - vqvae_loss: 0.4116\n",
      "Epoch 16/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.5935 - reconstruction_loss: 0.2004 - vqvae_loss: 0.3934\n",
      "Epoch 17/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.5919 - reconstruction_loss: 0.2008 - vqvae_loss: 0.3913\n",
      "Epoch 18/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.5766 - reconstruction_loss: 0.2006 - vqvae_loss: 0.3759\n",
      "Epoch 19/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.5513 - reconstruction_loss: 0.2011 - vqvae_loss: 0.3506\n",
      "Epoch 20/20\n",
      "1234/1234 [==============================] - 15s 12ms/step - loss: 0.5390 - reconstruction_loss: 0.2010 - vqvae_loss: 0.3379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x142701190>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae_trainer = VQVAETrainer(minmax_data_var, input_dim, latent_dim=24, num_embeddings=64)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "vqvae_trainer.fit(minmax_data, epochs=20, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 183ms/step\n",
      "tf.Tensor(0.8288087791973447, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.07296502,  0.02280979,  0.0343242 ,  0.05841448,  0.05666576,\n",
       "        0.03532025,  0.1964159 ,  0.07133903,  0.19771209,  0.23106134,\n",
       "        0.14412222,  0.09105127,  0.45276904,  0.37528221,  0.22548456,\n",
       "       -0.0366531 , -0.01320399, -0.03106298,  0.05971481,  0.21814827,\n",
       "       -0.12833317,  0.00554435,  0.38559891, -0.09135276,  0.04316327,\n",
       "        0.00863358,  0.19789152,  0.43170157,  0.20338359, -0.02806171,\n",
       "        0.34877901])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.1811023079320415, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.79384593e-01,  1.32812793e-01,  5.38383650e-02, -3.99978604e-02,\n",
       "       -3.63099696e-02, -1.06013122e-02, -3.88241646e-02,  2.99879074e-02,\n",
       "       -1.27910368e-04, -7.76650331e-02,  7.14738901e-03,  2.17432974e-01,\n",
       "        3.03707445e-02,  8.19061781e-03, -4.00972613e-03,  1.18111933e-01,\n",
       "       -7.35817205e-02, -6.52480002e-02, -4.18237320e-03,  2.98852439e-02,\n",
       "       -2.29808087e-02,  1.92722409e-01, -1.43514805e-01, -5.78668094e-02,\n",
       "       -5.52344612e-02, -5.37667771e-02, -1.21667319e-01,  1.79495054e-01,\n",
       "        8.83952903e-02, -1.64837477e-02, -3.80762344e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.24192150208687285, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01101071,  0.01132696,  0.00421246,  0.03390749,  0.02269675,\n",
       "        0.02621146,  0.20731449, -0.06223147,  0.00300397, -0.05127484,\n",
       "       -0.09101378, -0.07770947, -0.07123869, -0.0280389 , -0.09028562,\n",
       "        0.21268374, -0.03036263, -0.06436133,  0.06549198,  0.03245574,\n",
       "        0.13635737, -0.11874794, -0.12428877, -0.12539104,  0.08831251,\n",
       "       -0.12653904, -0.27718656, -0.11292468, -0.00385188,  0.09451216,\n",
       "        0.16302988])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.4108085806116736, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.30377722, -0.18342546, -0.11859993, -0.07207467,  0.04218171,\n",
       "       -0.10970465, -0.21080129,  0.04130792, -0.05337427, -0.02547207,\n",
       "       -0.0456536 , -0.06571311, -0.08542532, -0.07356542, -0.0106462 ,\n",
       "       -0.09283155, -0.03882729, -0.01294657, -0.02268951, -0.04820715,\n",
       "       -0.18353871, -0.17516484, -0.30750087,  0.15385635, -0.1555787 ,\n",
       "       -0.04015701,  0.33678185, -0.15259054, -0.0229454 , -0.07598557,\n",
       "       -0.00159633])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.1984593356148823, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.00594471e-01,  1.52599450e-01,  3.69352638e-02, -3.94204920e-02,\n",
       "       -4.98201645e-02, -8.94516358e-03, -8.74099848e-02, -1.72412858e-02,\n",
       "        6.33721006e-03, -1.25789212e-02,  6.69690703e-02, -5.80691353e-02,\n",
       "        1.66694070e-04, -2.59871725e-02, -3.27823070e-02, -5.85482748e-02,\n",
       "        9.54180709e-03, -1.20600139e-02, -1.66929055e-02,  1.22286258e-02,\n",
       "       -1.97128448e-01, -5.93678581e-02, -9.96613110e-02,  1.91520188e-01,\n",
       "        1.45722973e-02, -4.06092871e-02, -8.83456060e-03,  3.19480884e-01,\n",
       "        4.39413278e-03, -4.56822477e-02,  1.56533326e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.04702206327228253, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.70070185e-02,  2.17687027e-02, -1.95028277e-02,  5.71734515e-05,\n",
       "       -6.76814703e-03,  7.42418527e-03, -5.60995834e-02,  1.61530403e-03,\n",
       "        1.66885430e-02, -6.18491784e-03, -1.46265588e-02, -7.61695060e-02,\n",
       "        8.24678869e-02, -7.02794954e-02,  2.97272125e-03, -7.77413514e-02,\n",
       "       -1.40072392e-01, -3.46709274e-02, -1.23357704e-04, -3.88751692e-02,\n",
       "       -6.88999915e-02, -2.45051583e-02, -1.10172126e-02, -8.67303170e-03,\n",
       "       -4.66354168e-02, -2.96936200e-02, -5.97643014e-02,  5.25420312e-03,\n",
       "       -1.24311271e-02, -6.61710270e-02, -1.49516335e-02])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.17771465611751908, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02707574,  0.029754  , -0.01640784,  0.02132177,  0.01687611,\n",
       "        0.01998234,  0.02974671,  0.12236252, -0.00762449,  0.01145363,\n",
       "       -0.00718201,  0.00840676,  0.01077392, -0.08384768, -0.014951  ,\n",
       "       -0.0230625 ,  0.02150541, -0.03486962,  0.00493722, -0.02118344,\n",
       "       -0.17891702, -0.05145358,  0.37566718, -0.04651881,  0.11373691,\n",
       "       -0.08822613, -0.06178764, -0.00666617,  0.06030532, -0.12802459,\n",
       "       -0.06688639])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.05058079556143657, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01760599,  0.01375847,  0.00193569,  0.06058574,  0.06015563,\n",
       "        0.02925587, -0.0657935 , -0.00235815,  0.03613812, -0.05625992,\n",
       "        0.0160466 ,  0.01661509,  0.02515731, -0.0490436 ,  0.04063004,\n",
       "        0.01030864,  0.03697763, -0.01943846,  0.01558371,  0.04593276,\n",
       "       -0.16446441,  0.01945713, -0.01859915,  0.05376718, -0.05203467,\n",
       "       -0.00019724, -0.03561676,  0.02119419,  0.06357163,  0.07553553,\n",
       "        0.03500381])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.22195967253270318, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.19004071,  0.16358486,  0.02013358, -0.08104482, -0.06363776,\n",
       "       -0.05238323, -0.06050818, -0.16074469, -0.01523175, -0.02970979,\n",
       "       -0.01149831, -0.07046002, -0.06902711,  0.19206313,  0.00942331,\n",
       "       -0.12781534,  0.06749238, -0.03240734,  0.0024076 ,  0.00053159,\n",
       "        0.22236615,  0.0025933 , -0.07235671, -0.1174359 , -0.10256966,\n",
       "        0.03402063, -0.09708634,  0.2142326 , -0.08331954, -0.03847362,\n",
       "       -0.0117967 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "tf.Tensor(0.27556630014996986, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.11432953,  0.11463847, -0.00816755, -0.03665322, -0.01779703,\n",
       "       -0.02831986, -0.06308534, -0.01941224, -0.01385842, -0.03888438,\n",
       "       -0.02848687, -0.02838322, -0.05285966,  0.10116892, -0.02650351,\n",
       "       -0.00965606,  0.14330994, -0.03708725, -0.00544462,  0.0122146 ,\n",
       "        0.20655196, -0.09278547,  0.25124592, -0.02474644, -0.08187613,\n",
       "       -0.06080631, -0.08068548, -0.05355689, -0.04894722,  0.24655534,\n",
       "        0.35648194])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trained_vqvae_model = vqvae_trainer.vqvae\n",
    "idx = np.random.choice(len(test_minmax), 10)\n",
    "reconstructions_test = trained_vqvae_model.predict(test_minmax[idx])\n",
    "def difference(reconstruct, ground):\n",
    "    for i in range(len(reconstruct)):\n",
    "        print(tf.reduce_mean((ground[i] - reconstruct[i]) ** 2) / minmax_data_var)\n",
    "        display(reconstruct[i] - ground[i])\n",
    "        print('-------------------')\n",
    "difference(reconstructions_test, test_minmax[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping the cell id back to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157898/157898 [==============================] - 212s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "encoder = vqvae_trainer.vqvae.get_layer(\"encoder\")\n",
    "quantizer = vqvae_trainer.vqvae.get_layer(\"vector_quantizer\")\n",
    "\n",
    "encoded_outputs = encoder.predict(minmax_data)\n",
    "codebook_indices = quantizer.get_code_indices(encoded_outputs)\n",
    "codebook_indices = codebook_indices.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n",
    "cell number: 64  \n",
    "cell ratio: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['cell_id'] = codebook_indices\n",
    "def counting_cell(df):\n",
    "    idx, count = np.unique(df['cell_id'], return_counts=True)\n",
    "    total_num = len(np.unique(codebook_indices))\n",
    "    x = np.zeros(total_num)\n",
    "    for i in range(len(idx)):\n",
    "        x[idx[i]] = count[i]\n",
    "    ratio_x = x/sum(x)\n",
    "    x = np.concatenate([x, ratio_x])\n",
    "    x_cols = [str(i) for i in range(total_num)] + ['r'+str(i) for i in range(total_num)]\n",
    "    return pd.DataFrame([x], columns=x_cols)\n",
    "\n",
    "df = df_all.groupby('user_id').apply(counting_cell)\n",
    "id_idx = [i[0] for i in df.index]\n",
    "df.index = id_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('data/sick.csv')\n",
    "y['file_flow_id'] = y['file_flow_id'].apply(lambda x: re.search(user_id_pattern, x).group())\n",
    "y['label'] = y['label'].apply(lambda x: 1 if x == 'Sick' else 0)\n",
    "y.index = y['file_flow_id'].to_numpy()\n",
    "y = y.drop(columns='file_flow_id')\n",
    "df = pd.concat([df, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>r53</th>\n",
       "      <th>r54</th>\n",
       "      <th>r55</th>\n",
       "      <th>r56</th>\n",
       "      <th>r57</th>\n",
       "      <th>r58</th>\n",
       "      <th>r59</th>\n",
       "      <th>r60</th>\n",
       "      <th>r61</th>\n",
       "      <th>r62</th>\n",
       "      <th>r63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EU_035</th>\n",
       "      <td>1216.0</td>\n",
       "      <td>2247.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>2434.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>1259.0</td>\n",
       "      <td>3629.0</td>\n",
       "      <td>4341.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>2913.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>0.048258</td>\n",
       "      <td>0.017811</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.029690</td>\n",
       "      <td>0.015304</td>\n",
       "      <td>0.066011</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.020114</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>0.018875</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>0.005422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_047</th>\n",
       "      <td>1646.0</td>\n",
       "      <td>6143.0</td>\n",
       "      <td>6085.0</td>\n",
       "      <td>1322.0</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>4973.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4801.0</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>2575.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>761.0</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>15288.0</td>\n",
       "      <td>13726.0</td>\n",
       "      <td>11835.0</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>10239.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.015342</td>\n",
       "      <td>0.016493</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.024844</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>0.066910</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.031233</td>\n",
       "      <td>0.015712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_021</th>\n",
       "      <td>782.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>819.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.010411</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.013227</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.016232</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.044702</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>0.025540</td>\n",
       "      <td>0.017995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_014</th>\n",
       "      <td>280.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>789.0</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010385</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.032105</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.039007</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.055344</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.026976</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>0.022416</td>\n",
       "      <td>0.008992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_022</th>\n",
       "      <td>120.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>2699.0</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.045896</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.016496</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.060465</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.044230</td>\n",
       "      <td>0.019812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_043</th>\n",
       "      <td>498.0</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>4909.0</td>\n",
       "      <td>3337.0</td>\n",
       "      <td>1693.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>808.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.066143</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.018481</td>\n",
       "      <td>0.019144</td>\n",
       "      <td>0.005349</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.009289</td>\n",
       "      <td>0.025626</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.095696</td>\n",
       "      <td>0.024819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_034</th>\n",
       "      <td>242.0</td>\n",
       "      <td>6835.0</td>\n",
       "      <td>2377.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>764.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>3036.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>4595.0</td>\n",
       "      <td>3094.0</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>0.031945</td>\n",
       "      <td>0.035048</td>\n",
       "      <td>0.039388</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>0.035301</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.006288</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>0.020292</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>0.007616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU_031</th>\n",
       "      <td>4728.0</td>\n",
       "      <td>4891.0</td>\n",
       "      <td>3417.0</td>\n",
       "      <td>1528.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>2903.0</td>\n",
       "      <td>2715.0</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>2985.0</td>\n",
       "      <td>2982.0</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>4033.0</td>\n",
       "      <td>2748.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.017019</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.027556</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.013391</td>\n",
       "      <td>0.016498</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.005742</td>\n",
       "      <td>0.018192</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.007701</td>\n",
       "      <td>0.003332</td>\n",
       "      <td>0.017464</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.017013</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0.042990</td>\n",
       "      <td>0.014538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1       2       3       4       5       6       7   \n",
       "EU_035  1216.0  2247.0   574.0   113.0   167.0  1407.0   494.0   744.0  \\\n",
       "EU_047  1646.0  6143.0  6085.0  1322.0  1718.0  4973.0   250.0  4801.0   \n",
       "EU_021   782.0   523.0   819.0  2015.0   289.0   198.0  1635.0  3400.0   \n",
       "EU_014   280.0   245.0   176.0    57.0    26.0   137.0    38.0   293.0   \n",
       "EU_022   120.0   296.0  2699.0  1751.0   669.0   318.0    90.0   817.0   \n",
       "EU_043   498.0  1437.0  4909.0  3337.0  1693.0   647.0    93.0   964.0   \n",
       "EU_034   242.0  6835.0  2377.0   710.0   764.0  2056.0    15.0  1442.0   \n",
       "EU_031  4728.0  4891.0  3417.0  1528.0   967.0  1457.0  2903.0  2715.0   \n",
       "\n",
       "             8       9      10      11      12      13       14       15   \n",
       "EU_035  2434.0   227.0   339.0   616.0   652.0  1259.0   3629.0   4341.0  \\\n",
       "EU_047  5916.0   958.0  2575.0   224.0   761.0  1766.0  15288.0  13726.0   \n",
       "EU_021   735.0  1155.0   273.0  1089.0   251.0   687.0    277.0     84.0   \n",
       "EU_014   210.0    21.0    25.0    29.0   176.0   146.0    789.0   1204.0   \n",
       "EU_022   329.0   505.0  1268.0    68.0    38.0   306.0    419.0    315.0   \n",
       "EU_043  1009.0  2268.0  1525.0    98.0   167.0   290.0   1196.0    870.0   \n",
       "EU_034  3036.0   846.0  1787.0    24.0   106.0  1511.0   4595.0   3094.0   \n",
       "EU_031  3460.0  1663.0  1021.0  2985.0  2982.0  1836.0   4033.0   2748.0   \n",
       "\n",
       "             16      17       18      19  ...       r44       r45       r46   \n",
       "EU_035    948.0   980.0   2913.0   241.0  ...  0.007681  0.009401  0.009255  \\\n",
       "EU_047  11835.0  1604.0  10239.0   476.0  ...  0.002206  0.023228  0.015342   \n",
       "EU_021   1319.0   309.0    123.0  1821.0  ...  0.003018  0.010411  0.005225   \n",
       "EU_014    625.0   321.0    932.0    25.0  ...  0.010385  0.009372  0.003040   \n",
       "EU_022    721.0   561.0    135.0   119.0  ...  0.000474  0.045896  0.016905   \n",
       "EU_043    808.0   124.0    470.0   152.0  ...  0.003446  0.066143  0.008265   \n",
       "EU_034   1523.0   834.0    749.0    15.0  ...  0.001450  0.024836  0.031945   \n",
       "EU_031   1186.0  1410.0   1670.0  1364.0  ...  0.013842  0.017019  0.012637   \n",
       "\n",
       "             r47       r48       r49       r50       r51       r52       r53   \n",
       "EU_035  0.048258  0.017811  0.004023  0.029690  0.015304  0.066011  0.004285  \\\n",
       "EU_047  0.016493  0.014451  0.010723  0.024844  0.023586  0.066910  0.001876   \n",
       "EU_021  0.013227  0.011870  0.000799  0.008382  0.012022  0.016232  0.003094   \n",
       "EU_014  0.032105  0.017920  0.004749  0.039007  0.015324  0.055344  0.002470   \n",
       "EU_022  0.012985  0.005423  0.008738  0.003087  0.007284  0.009996  0.000702   \n",
       "EU_043  0.007735  0.015819  0.002361  0.011747  0.018481  0.019144  0.005349   \n",
       "EU_034  0.035048  0.039388  0.003265  0.013954  0.035301  0.030799  0.000416   \n",
       "EU_031  0.027556  0.019339  0.002997  0.013391  0.016498  0.028316  0.005742   \n",
       "\n",
       "             r54       r55       r56       r57       r58       r59       r60   \n",
       "EU_035  0.020114  0.002915  0.011835  0.000189  0.003994  0.018875  0.003848  \\\n",
       "EU_047  0.002991  0.005872  0.008513  0.000224  0.003683  0.023700  0.004147   \n",
       "EU_021  0.005694  0.009625  0.015091  0.008852  0.044702  0.005225  0.049166   \n",
       "EU_014  0.007409  0.000887  0.026976  0.000823  0.004053  0.012855  0.004496   \n",
       "EU_022  0.001241  0.016496  0.003430  0.001829  0.060465  0.004426  0.056300   \n",
       "EU_043  0.005337  0.028927  0.007385  0.002373  0.019867  0.009289  0.025626   \n",
       "EU_034  0.001572  0.008610  0.006288  0.000071  0.008539  0.020292  0.009208   \n",
       "EU_031  0.018192  0.005297  0.007701  0.003332  0.017464  0.009364  0.017013   \n",
       "\n",
       "             r61       r62       r63  \n",
       "EU_035  0.009211  0.015916  0.005422  \n",
       "EU_047  0.004350  0.031233  0.015712  \n",
       "EU_021  0.011109  0.025540  0.017995  \n",
       "EU_014  0.020137  0.022416  0.008992  \n",
       "EU_022  0.002597  0.044230  0.019812  \n",
       "EU_043  0.004000  0.095696  0.024819  \n",
       "EU_034  0.002485  0.043151  0.007616  \n",
       "EU_031  0.043854  0.042990  0.014538  \n",
       "\n",
       "[8 rows x 128 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.permutation(len(df))\n",
    "train = df.iloc[idx[:int(len(df)*0.8)]]\n",
    "test = df.iloc[idx[int(len(df)*0.8):]]\n",
    "test.iloc[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(32),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(fused=False),\n",
    "    layers.Dense(16),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.BatchNormalization(fused=False),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5746 - accuracy: 0.7188\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5547 - accuracy: 0.7500\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5391 - accuracy: 0.7500\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5269 - accuracy: 0.7188\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5169 - accuracy: 0.7188\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5083 - accuracy: 0.7500\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5016 - accuracy: 0.7500\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4950 - accuracy: 0.7500\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4886 - accuracy: 0.7500\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4823 - accuracy: 0.7500\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4761 - accuracy: 0.7188\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4700 - accuracy: 0.6875\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.6875\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4595 - accuracy: 0.6875\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4547 - accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train.iloc[:,:-1], train['label'], epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5335 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5334621667861938, 0.75]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test.iloc[:,:-1], test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result\n",
    "In this case, the number of the grouping should be consider as follow. First, How many kind of cell should be presented. Second, is it presenting well enough. There could be more method to estimate these two. \n",
    "   \n",
    "Feature engineer can consider more piror distribution which means target important cell group by knowing domain knowledge. Eventually, I put all the features on, and choice nn model to learn.  \n",
    "  \n",
    "Then the classification model can do one more normalization but due to the lack of the instance, I decided not to do it here.  \n",
    "  \n",
    "Overall, the test accuracy flucuated between 0.375 to 0.75, mostly around 0.6 which is not to bad since the training accuaracy is around 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
